{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "43818c16bbce4d4094c8ac193f9cfece": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DropdownModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DropdownModel",
            "_options_labels": [
              "CPU",
              "AUTO"
            ],
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "DropdownView",
            "description": "Device:",
            "description_tooltip": null,
            "disabled": false,
            "index": 1,
            "layout": "IPY_MODEL_f504e0755cd54b8abbc3192b9a69684a",
            "style": "IPY_MODEL_2982e58f4bc942f1b5e35bd01a1ce680"
          }
        },
        "f504e0755cd54b8abbc3192b9a69684a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2982e58f4bc942f1b5e35bd01a1ce680": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Tymass/chess-player/blob/yolo-training/YOLOv9_quantization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# YOLOv9 Quantization based on [this](https://docs.openvino.ai/2024/notebooks/yolov9-optimization-with-output.html) article."
      ],
      "metadata": {
        "id": "cE6oiRY2nKiF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### INSTALLATIONS"
      ],
      "metadata": {
        "id": "eKGMPqglAGd8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zuPaucrN2dyN",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "!pip install torch torchvision\n",
        "!pip install ultralytics\n",
        "!pip install roboflow\n",
        "\n",
        "import platform\n",
        "if platform.system() != \"Windows\":\n",
        "    %pip install -q \"matplotlib>=3.4\"\n",
        "else:\n",
        "    %pip install -q \"matplotlib>=3.4,<3.7\"\n",
        "\n",
        "!pip install -q \"nncf>=2.8.1\"\n",
        "!pip install -q \"openvino>=2023.3.0\" \"opencv-python\" \"seaborn\" \"pandas\" \"scikit-learn\" \"torch\" \"torchvision\"  --extra-index-url https://download.pytorch.org/whl/cpu\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### IMPORTS"
      ],
      "metadata": {
        "id": "Nwis2slKAJSu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from roboflow import Roboflow\n",
        "import ipywidgets as widgets\n",
        "import sys\n",
        "from pathlib import Path\n",
        "import torch\n",
        "import nncf\n",
        "\n",
        "import openvino as ov\n",
        "\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "\n",
        "from collections import namedtuple\n",
        "import yaml\n",
        "\n",
        "from typing import List, Tuple\n",
        "import time\n",
        "import os\n",
        "\n",
        "from google.colab import userdata"
      ],
      "metadata": {
        "id": "UJdvsiGE6quN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### CLONE YOLOv9 REPO"
      ],
      "metadata": {
        "id": "jSuAr_lQAwkh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sys.path.append(\"../utils\")\n",
        "\n",
        "if not Path('yolov9').exists():\n",
        "    !git clone https://github.com/WongKinYiu/yolov9\n",
        "%cd yolov9\n",
        "\n",
        "from models.experimental import attempt_load\n",
        "from models.yolo import Detect, DualDDetect\n",
        "from utils.general import yaml_save, yaml_load\n",
        "from utils.augmentations import letterbox\n",
        "from utils.dataloaders import create_dataloader\n",
        "from utils.general import colorstr\n",
        "from utils.general import scale_boxes, non_max_suppression\n",
        "from utils.plots import Annotator, colors"
      ],
      "metadata": {
        "id": "wFbO2UNx1cIe",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### FUNCTION DEFINITIONS"
      ],
      "metadata": {
        "id": "pGG7AqVkAMxq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_image(img0: np.ndarray):\n",
        "    \"\"\"\n",
        "    Preprocess image according to YOLOv9 input requirements.\n",
        "    Takes image in np.array format, resizes it to specific size using letterbox resize, converts color space from BGR (default in OpenCV) to RGB and changes data layout from HWC to CHW.\n",
        "\n",
        "    Parameters:\n",
        "      img0 (np.ndarray): image for preprocessing\n",
        "    Returns:\n",
        "      img (np.ndarray): image after preprocessing\n",
        "      img0 (np.ndarray): original image\n",
        "    \"\"\"\n",
        "    # resize\n",
        "    img = letterbox(img0, auto=False)[0]\n",
        "\n",
        "    # Convert\n",
        "    img = img.transpose(2, 0, 1)\n",
        "    img = np.ascontiguousarray(img)\n",
        "    return img, img0\n",
        "\n",
        "def prepare_input_tensor(image: np.ndarray):\n",
        "    \"\"\"\n",
        "    Converts preprocessed image to tensor format according to YOLOv9 input requirements.\n",
        "    Takes image in np.array format with unit8 data in [0, 255] range and converts it to torch.Tensor object with float data in [0, 1] range\n",
        "\n",
        "    Parameters:\n",
        "      image (np.ndarray): image for conversion to tensor\n",
        "    Returns:\n",
        "      input_tensor (torch.Tensor): float tensor ready to use for YOLOv9 inference\n",
        "    \"\"\"\n",
        "    input_tensor = image.astype(np.float32)  # uint8 to fp16/32\n",
        "    input_tensor /= 255.0  # 0 - 255 to 0.0 - 1.0\n",
        "\n",
        "    if input_tensor.ndim == 3:\n",
        "        input_tensor = np.expand_dims(input_tensor, 0)\n",
        "    return input_tensor\n",
        "\n",
        "def transform_fn(data_item):\n",
        "    \"\"\"\n",
        "    Quantization transform function. Extracts and preprocess input data from dataloader item for quantization.\n",
        "    Parameters:\n",
        "       data_item: Tuple with data item produced by DataLoader during iteration\n",
        "    Returns:\n",
        "        input_tensor: Input data for quantization\n",
        "    \"\"\"\n",
        "    img = data_item[0].numpy()\n",
        "    input_tensor = prepare_input_tensor(img)\n",
        "    return input_tensor\n",
        "\n",
        "def draw_boxes(predictions: np.ndarray, input_shape: Tuple[int], image: np.ndarray, names: List[str]):\n",
        "    \"\"\"\n",
        "    Utility function for drawing predicted bounding boxes on image\n",
        "    Parameters:\n",
        "        predictions (np.ndarray): list of detections with (n,6) shape, where n - number of detected boxes in format [x1, y1, x2, y2, score, label]\n",
        "        image (np.ndarray): image for boxes visualization\n",
        "        names (List[str]): list of names for each class in dataset\n",
        "        colors (Dict[str, int]): mapping between class name and drawing color\n",
        "    Returns:\n",
        "        image (np.ndarray): box visualization result\n",
        "    \"\"\"\n",
        "    if not len(predictions):\n",
        "        return image\n",
        "\n",
        "    annotator = Annotator(image, line_width=1, example=str(names))\n",
        "    # Rescale boxes from input size to original image size\n",
        "    predictions[:, :4] = scale_boxes(input_shape[2:], predictions[:, :4], image.shape).round()\n",
        "\n",
        "    # Write results\n",
        "    for *xyxy, conf, cls in reversed(predictions):\n",
        "        label = f'{names[int(cls)]} {conf:.2f}'\n",
        "        annotator.box_label(xyxy, label, color=colors(int(cls), True))\n",
        "    return image\n",
        "\n",
        "def detect(model: ov.Model, image_path: Path, conf_thres: float = 0.25, iou_thres: float = 0.45, classes: List[int] = None, agnostic_nms: bool = False):\n",
        "    \"\"\"\n",
        "    OpenVINO YOLOv9 model inference function. Reads image, preprocess it, runs model inference and postprocess results using NMS.\n",
        "    Parameters:\n",
        "        model (Model): OpenVINO compiled model.\n",
        "        image_path (Path): input image path.\n",
        "        conf_thres (float, *optional*, 0.25): minimal accepted confidence for object filtering\n",
        "        iou_thres (float, *optional*, 0.45): minimal overlap score for removing objects duplicates in NMS\n",
        "        classes (List[int], *optional*, None): labels for prediction filtering, if not provided all predicted labels will be used\n",
        "        agnostic_nms (bool, *optional*, False): apply class agnostic NMS approach or not\n",
        "    Returns:\n",
        "       pred (List): list of detections with (n,6) shape, where n - number of detected boxes in format [x1, y1, x2, y2, score, label]\n",
        "       orig_img (np.ndarray): image before preprocessing, can be used for results visualization\n",
        "       inpjut_shape (Tuple[int]): shape of model input tensor, can be used for output rescaling\n",
        "    \"\"\"\n",
        "    if isinstance(image_path, np.ndarray):\n",
        "        img = image_path\n",
        "    else:\n",
        "        img = np.array(Image.open(image_path))\n",
        "    preprocessed_img, orig_img = preprocess_image(img)\n",
        "    input_tensor = prepare_input_tensor(preprocessed_img)\n",
        "    predictions = torch.from_numpy(model(input_tensor)[0])\n",
        "    pred = non_max_suppression(predictions, conf_thres, iou_thres, classes=classes, agnostic=agnostic_nms)\n",
        "    return pred, orig_img, input_tensor.shape\n",
        "\n",
        "def transform_fn(data_item):\n",
        "    \"\"\"\n",
        "    Quantization transform function. Extracts and preprocess input data from dataloader item for quantization.\n",
        "    Parameters:\n",
        "       data_item: Tuple with data item produced by DataLoader during iteration\n",
        "    Returns:\n",
        "        input_tensor: Input data for quantization\n",
        "    \"\"\"\n",
        "    img = data_item[0].numpy()\n",
        "    input_tensor = prepare_input_tensor(img)\n",
        "    return input_tensor\n"
      ],
      "metadata": {
        "id": "gAAXrEhP60Xf"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### DOWNLOAD DATASET"
      ],
      "metadata": {
        "id": "ErtRSpTFAruA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "API_key = userdata.get('API_key')\n",
        "rf = Roboflow(api_key=API_key)\n",
        "project = rf.workspace(\"tymek-byrwa-1p3fh\").project(\"chesspiecesdetection-y9ljv\")\n",
        "project_version = 6\n",
        "version = project.version(project_version)\n",
        "dataset = version.download(\"yolov9\")"
      ],
      "metadata": {
        "id": "50ychIsG4CYc",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "YOU NEED TO UPLOAD WEIGHTS TO */content/yolov9/model*"
      ],
      "metadata": {
        "id": "EDxQIujdBPJ9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "DATA_DIR = Path(f'/content/yolov9/chessPiecesDetection-{project_version}/')\n",
        "MODEL_DIR = Path(\"/content/yolov9/model/\")\n",
        "MODEL_DIR.mkdir(exist_ok=True)\n",
        "DATA_DIR.mkdir(exist_ok=True)"
      ],
      "metadata": {
        "id": "qWnSavh4MtY0"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### LOAD MODEL"
      ],
      "metadata": {
        "id": "Qd5IpAEEB_1W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "weights = MODEL_DIR / \"best.pt\"\n",
        "ov_model_path = MODEL_DIR / weights.name.replace(\".pt\", \"_openvino_model\") / weights.name.replace(\".pt\", \".xml\")\n",
        "\n",
        "\n",
        "if not ov_model_path.exists():\n",
        "    model = attempt_load(weights, device=\"cpu\", inplace=True, fuse=True)\n",
        "    metadata = {'stride': int(max(model.stride)), 'names': model.names}\n",
        "\n",
        "    model.eval()\n",
        "    for k, m in model.named_modules():\n",
        "        if isinstance(m, (Detect, DualDDetect)):\n",
        "            m.inplace = False\n",
        "            m.dynamic = True\n",
        "            m.export = True\n",
        "\n",
        "    example_input = torch.zeros((1, 3, 640, 640))\n",
        "    model(example_input)\n",
        "\n",
        "    ov_model = ov.convert_model(model, example_input=example_input)\n",
        "\n",
        "    # specify input and output names for compatibility with yolov9 repo interface\n",
        "    ov_model.outputs[0].get_tensor().set_names({\"output0\"})\n",
        "    ov_model.inputs[0].get_tensor().set_names({\"images\"})\n",
        "    ov.save_model(ov_model, ov_model_path)\n",
        "    # save metadata\n",
        "    yaml_save(ov_model_path.parent / weights.name.replace(\".pt\", \".yaml\"), metadata)\n",
        "else:\n",
        "    metadata = yaml_load(ov_model_path.parent / weights.name.replace(\".pt\", \".yaml\"))"
      ],
      "metadata": {
        "id": "nFxlKUJt1ou2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### CREATE DATALOADER\n"
      ],
      "metadata": {
        "id": "tk4IOuSsA_XH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data[TASK]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "Q5v9MeJdsu0o",
        "outputId": "c3fe7594-3005-49ae-d5aa-b76b5d4640d7"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'chessPiecesDetection-6/valid/images'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# read dataset config\n",
        "DATA_CONFIG = DATA_DIR / 'data.yaml'\n",
        "YOLO_PATH = Path('/content/yolov9/')\n",
        "with open(DATA_CONFIG) as f:\n",
        "    data = yaml.load(f, Loader=yaml.SafeLoader)\n",
        "\n",
        "# Dataloader\n",
        "TASK = 'val'  # path to train/val/test images\n",
        "Option = namedtuple('Options', ['single_cls'])  # imitation of commandline provided options for single class evaluation\n",
        "opt = Option(False)\n",
        "\n",
        "# Specify dataset for accuracy control Qunatization\n",
        "if TASK == 'test':\n",
        "  DATA_PATH = str(DATA_DIR / data[TASK])\n",
        "else:\n",
        "  DATA_PATH = str(YOLO_PATH / data[TASK])\n",
        "\n",
        "dataloader = create_dataloader(\n",
        "    DATA_PATH, 640, 1, 32, opt, pad=0.5,\n",
        "    prefix=colorstr(f'{TASK}: ')\n",
        ")[0]\n",
        "\n",
        "\n",
        "quantization_dataset = nncf.Dataset(dataloader, transform_fn)"
      ],
      "metadata": {
        "id": "fTyIFJ253kAZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### PERFORM QUANTIZATION"
      ],
      "metadata": {
        "id": "0UQ3hFa4B387"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ov_int8_model_path = MODEL_DIR / weights.name.replace(\".pt\",\"_int8_openvino_model\") / weights.name.replace(\".pt\", \"_int8.xml\")\n",
        "\n",
        "if not ov_int8_model_path.exists():\n",
        "    quantized_model = nncf.quantize(ov_model, quantization_dataset, preset=nncf.QuantizationPreset.MIXED)\n",
        "\n",
        "    ov.save_model(quantized_model, ov_int8_model_path)\n",
        "    yaml_save(ov_int8_model_path.parent / weights.name.replace(\".pt\", \"_int8.yaml\"), metadata)"
      ],
      "metadata": {
        "id": "xp1R_dXe40-v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### LOAD QUANTIZED MODEL"
      ],
      "metadata": {
        "id": "w-8J3VCUCcVq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "core = ov.Core()\n",
        "device = widgets.Dropdown(\n",
        "    options=core.available_devices + [\"AUTO\"],\n",
        "    value='AUTO',\n",
        "    description='Device:',\n",
        "    disabled=False,\n",
        ")\n",
        "\n",
        "quantized_model = core.read_model(ov_int8_model_path)\n",
        "\n",
        "if device.value != \"CPU\":\n",
        "    quantized_model.reshape({0: [1, 3, 640, 640]})\n",
        "\n",
        "compiled_model = core.compile_model(quantized_model, device.value)"
      ],
      "metadata": {
        "id": "DVAC1uOm8IA_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "43818c16bbce4d4094c8ac193f9cfece",
            "f504e0755cd54b8abbc3192b9a69684a",
            "2982e58f4bc942f1b5e35bd01a1ce680"
          ]
        },
        "id": "MNCwNHmVtlVO",
        "outputId": "dfca04d6-aba9-4287-f5c3-0cf8365767a4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Dropdown(description='Device:', index=1, options=('CPU', 'AUTO'), value='AUTO')"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "43818c16bbce4d4094c8ac193f9cfece"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### TEST QUNATIZED MODEL"
      ],
      "metadata": {
        "id": "RtmbUhX_BwN7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "boxes, image, input_shape = detect(compiled_model,\"/content/yolov9/chessPiecesDetection-5/test/images/1714395096705_jpg.rf.b25180e3b0aa27d77b1dc05a27a70caa.jpg\")\n",
        "NAMES = metadata[\"names\"]\n",
        "image_with_boxes = draw_boxes(boxes[0], input_shape, image, NAMES)\n",
        "# visualize results\n",
        "Image.fromarray(image_with_boxes)"
      ],
      "metadata": {
        "id": "kRu9q-XH8DIY",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Benchmarks"
      ],
      "metadata": {
        "id": "PGczwfEMAkwD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!benchmark_app -m $ov_model_path -shape \"[1,3,640,640]\" -d $device.value -api async -t 15"
      ],
      "metadata": {
        "id": "qsmQ501v8khK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!benchmark_app -m $ov_int8_model_path -shape \"[1,3,640,640]\" -d $device.value -api async -t 15"
      ],
      "metadata": {
        "id": "KuAm91tX833p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate predictions time\n",
        "def predictions_time(path):\n",
        "    try:\n",
        "        if not os.path.isdir(path):\n",
        "            print(\"Wrong path.\")\n",
        "            return\n",
        "        files = os.listdir(path)\n",
        "\n",
        "        for file in files:\n",
        "            t1 = time.time()\n",
        "\n",
        "            results = model.predict(path + '/' + file)\n",
        "\n",
        "            t2 = time.time()\n",
        "            dt = t2 - t1\n",
        "\n",
        "            print(f\"Detection time: {dt:.4f} seconds\")\n",
        "\n",
        "\n",
        "    except Exception as e:\n",
        "        print(\"Wystąpił błąd:\", e)"
      ],
      "metadata": {
        "id": "NRwSD9I8tNZC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions_time('/content/yolov9/chessPiecesDetection-6/test')"
      ],
      "metadata": {
        "id": "v1CqSqEWtWS-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}